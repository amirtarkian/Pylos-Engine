# V5 Training: Potential Next Steps

## Current Status (as of ~242K games / 48% complete)

- **Value loss:** 0.29–0.32 (plateau since ~100K)
- **Policy loss:** ~0.16–0.17 (converged since ~50K games)
- **ELO:** oscillating 1550–1780, peak 1781 at step 235K
- **Architecture:** 10 residual blocks × 256 hidden, 92-dim rich observations

## Why Loss Is Plateauing

Value targets are raw game outcomes (+1/−1/0), assigned identically to every position in a game. This creates high label noise — an objectively losing position gets +1 if the opponent blunders later. With 64 MCTS iterations during self-play, this noise floor sits around MSE 0.25–0.30.

Policy loss converged early because the MCTS visit distribution is a smooth, well-defined target. Value loss is fundamentally harder because it depends on game outcome accuracy.

## Changes Applied (restart at step 242K)

### 1. Increase Batch Size (256 → 1024) ✅
Sampling 0.4% of the replay buffer per step was too sparse. At 1024, we sample 3.1% of the buffer (1024/32768), giving much smoother gradients and less per-step variance.

### 2. Reduce Replay Buffer (65536 → 32768) ✅
Faster turnover means the model trains on positions generated by its current (stronger) version rather than older, weaker play. Combined with larger batch size gives 3.1% sampling ratio.

### 3. Lower Temperature Threshold (15 → 10) ✅
15 moves of temperature=1 exploration was too much for Pylos (~25–40 total moves). Reducing to 10 keeps sufficient exploration while producing higher-quality self-play data.

### 4. Weight Value Loss Higher (1.0 → 1.5×) ✅
Policy loss at 0.17 is barely contributing gradient signal. Weighting value loss 1.5× pushes the optimizer toward better position evaluation — the head that still has room to improve.

### 5. Improved ELO Evaluation ✅ (already committed)
- eval_games: 60 → 200 (40 games per matchup instead of 6)
- MAX_EVAL_OPPONENTS: 10 → 5 (fewer, more meaningful comparisons)
- EVAL_SEARCH_ITERS: 32 → 64 (match training search depth)

## Deferred: Increase MCTS Search Iterations (64 → 128+)

This is the highest-leverage change for data quality but costs ~2× in self-play speed. Deferred to evaluate whether the other changes break through the plateau first. If value loss is still stuck at ~0.28 after 50K more games, increase search iterations next.

## Known Issue: Replay Buffer Not Persisted on Resume

When training is stopped and resumed, the replay buffer starts empty (only model weights + optimizer state are restored). This causes a transient loss spike after each resume. Not critical but worth fixing in a future iteration.

## What NOT to Change

- **Architecture** — 10 blocks × 256 has plenty of capacity for Pylos. The bottleneck is data quality, not model expressiveness.
- **Learning rate** — Cosine annealing from 0.001 → 0.00005 is fine. The schedule will naturally reduce LR as training progresses.
- **Weight decay** — 0.0001 is standard and not causing issues.
