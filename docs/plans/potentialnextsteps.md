# V5 Training: Potential Next Steps

## Current Status (as of ~175K games / 35% complete)

- **Value loss:** 0.25–0.30 (plateau)
- **Policy loss:** ~0.17 (converged since ~50K games)
- **ELO:** oscillating 1520–1730, peak 1730 at step 175K
- **Architecture:** 10 residual blocks × 256 hidden, 92-dim rich observations

## Why Loss Is Plateauing

Value targets are raw game outcomes (+1/−1/0), assigned identically to every position in a game. This creates high label noise — an objectively losing position gets +1 if the opponent blunders later. With 64 MCTS iterations during self-play, this noise floor sits around MSE 0.25–0.30.

Policy loss converged early because the MCTS visit distribution is a smooth, well-defined target. Value loss is fundamentally harder because it depends on game outcome accuracy.

## Recommended Changes (Priority Order)

### 1. Increase MCTS Search Iterations (64 → 128+)
**Impact: HIGH** — This is the single highest-leverage change.

Stronger search = higher-quality self-play = more accurate value targets = lower noise floor. Equivalent to giving the model a better teacher. Cost is ~2× slower self-play per game, but significantly more sample-efficient.

```yaml
training:
  search_iterations: 128  # was 64
```

### 2. Increase Batch Size (256 → 1024)
**Impact: MEDIUM-HIGH** — Essentially free, smooths loss curves.

Currently sampling 0.4% of the replay buffer per step. At 1024, you'd sample 1.6%, giving much smoother gradients and less per-step variance. GPU/MPS can handle this without slowdown.

```yaml
training:
  batch_size: 1024  # was 256
```

### 3. Lower Temperature Threshold (15 → 8–10)
**Impact: MEDIUM** — Reduces noise from random early-game moves.

15 moves of temperature=1 exploration is a lot for Pylos (~25–40 total moves). Reducing to 8–10 keeps sufficient exploration while producing higher-quality data.

```yaml
training:
  temp_threshold: 10  # was 15
```

### 4. Weight Value Loss Higher (1.0 → 1.5×)
**Impact: MEDIUM** — Focuses optimizer on the head that still has room to improve.

Policy loss at 0.17 is barely contributing gradient signal. Weighting value loss higher pushes the optimizer toward better position evaluation.

```python
# In train.py _train_batch():
(1.5 * vloss + ploss).backward()  # was (vloss + ploss)
```

### 5. Reduce Replay Buffer (65536 → 32768)
**Impact: LOW-MEDIUM** — Trains more on recent, higher-quality data.

Smaller buffer means faster turnover, so the model trains on positions generated by its current (stronger) version rather than older, weaker play.

```yaml
training:
  replay_buffer_size: 32768  # was 65536
```

## Bug Fix: Replay Buffer Not Persisted on Resume

When training is stopped and resumed, the replay buffer starts empty (only model weights + optimizer state are restored). This causes a transient loss spike after each resume. Consider saving/loading the replay buffer alongside the checkpoint.

## What NOT to Change

- **Architecture** — 10 blocks × 256 has plenty of capacity for Pylos. The bottleneck is data quality, not model expressiveness.
- **Learning rate** — Cosine annealing from 0.001 → 0.00005 is fine. The schedule will naturally reduce LR as training progresses.
- **Weight decay** — 0.0001 is standard and not causing issues.
